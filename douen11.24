{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP81s+/DUrzyiJ2VMlPrpnM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Y00nnye/Teamproject_23-2/blob/main/douen11.24\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R56qpPbzAx9q"
      },
      "outputs": [],
      "source": [
        "#0.사용자 지정함수 설정,라이브러리 임포트\n",
        "!pip install pandas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "!pip install keybert\n",
        "!pip install transformers\n",
        "!pip install kiwipiepy\n",
        "\n",
        "from keybert import KeyBERT\n",
        "from kiwipiepy import Kiwi\n",
        "from transformers import BertModel\n",
        "\n",
        "def print_obj(obj, name):\n",
        "    print(\"%s:\\n%s\\n\" % (name, obj))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#1.데이터 불러오기(데이터형확인)(dim,shape)\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "# Google Drive 마운트\n",
        "drive.mount('/content/drive')\n",
        "file_path = '/content/drive/MyDrive/4U/db/db.sheet.xlsx'\n",
        "dataset = pd.read_excel(file_path)\n",
        "\n",
        "# 데이터 확인\n",
        "print(dataset)\n",
        "print(dataset.dtypes)# 각 열의 데이터 타입 확인\n",
        "print(dataset.shape)# 데이터의 행과 열의 개수 확인\n",
        "\n",
        "#titleSet keywordSet나누기\n",
        "X = dataset.iloc[:, 1] #titleSet\n",
        "y1 = dataset.iloc[:, 2] #keywordSet1\n",
        "y2 = dataset.iloc[:, 3] #keywordSet2\n",
        "y3 = dataset.iloc[:, 4] #keywordSet3\n",
        "\n",
        "print(\"X_dataset\")\n",
        "print(X)\n",
        "print(\"X_shape\")\n",
        "print(X.shape)\n",
        "\n",
        "print(\"y1_dataset\")\n",
        "print(y1)\n",
        "print(\"y1_shape\")\n",
        "print(y1.shape)\n",
        "print(y1.dtype)\n",
        "\n",
        "print(\"y2_dataset\")\n",
        "print(y2)\n",
        "print(\"y3_dataset\")\n",
        "print(y3)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#2.kiwi로 형태소 분석 및 분해\n",
        "\n",
        "#필요한 라이브러리 불러오기(분해 이전)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "!pip install kiwipiepy\n",
        "from kiwipiepy import Kiwi\n",
        "\n",
        "from keybert import KeyBERT\n",
        "from transformers import BertModel\n",
        "\n",
        "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
        "kw_model = KeyBERT(model)\n",
        "\n",
        "import re\n",
        "\n",
        "# Kiwi 형태소 분석기 초기화\n",
        "kiwi = Kiwi()\n",
        "\n",
        "# 정규표현식을 사용하여 알파벳이 포함된 단어를 걸러내는 패턴\n",
        "alphabet_pattern = re.compile('[a-zA-Z]')\n",
        "\n",
        "# 각 행의 키워드를 저장할 리스트\n",
        "X_kiwi = []\n",
        "\n",
        "# 각 행을 처리하여 키워드 추출 후 저장\n",
        "for title in X:\n",
        "    # Kiwi 형태소 분석 수행\n",
        "    kiwi_result = kiwi.analyze(title)\n",
        "\n",
        "    # 명사 추출 함수\n",
        "    def noun_extractor_kiwi(kiwi_result):\n",
        "        nouns = []\n",
        "        for token, pos, _, _ in kiwi_result[0][0]:\n",
        "            if len(token) != 1 and (pos.startswith('N') or pos.startswith('SL')) and not alphabet_pattern.search(token):\n",
        "                nouns.append(token)\n",
        "        return nouns\n",
        "\n",
        "    # 명사 추출\n",
        "    nouns = noun_extractor_kiwi(kiwi_result)\n",
        "\n",
        "    # 추출된 명사를 X_kiwi에 추가\n",
        "    X_kiwi.append(nouns)\n",
        "\n",
        "for i in range(len(X_kiwi)):\n",
        "  print(X_kiwi[i])\n",
        "\n",
        "#KIWI=KIWI_COMBINED(목적; 코버트 사용을 위해 리스트에서 문자열로 JOIN\n",
        "# x 리스트에 있는 문자열들을 join하여 하나의 문자열로 합치기\n",
        "\n",
        "X_kiwi_combined = [' '.join(sentence) for sentence in X_kiwi]\n",
        "print(len(X_kiwi_combined))\n",
        "print(X_kiwi_combined)\n",
        "\n",
        "#k_kiwi값 엑셀에 자동저장(num=217,header가 추가됨)\n",
        "!pip install xlsxwriter\n",
        "df_kiwi = pd.DataFrame(X_kiwi)\n",
        "file_path = '/content/drive/MyDrive/4U/db/kiwi_auto.xlsx'\n",
        "with pd.ExcelWriter(file_path, engine='xlsxwriter') as writer:\n",
        "    df_kiwi.to_excel(writer, sheet_name='kiwi_auto', index=False)\n",
        "\n",
        "print(len(X_kiwi_combined))\n",
        "\n",
        "\n",
        "\n",
        "#3.중요도 불러오기(keybert사용)\n",
        "\n",
        "#weight_set초기화\n",
        "weight_set=[]\n",
        "\n",
        "#weight_set에서 각 문자열을 가져와서 단어의 개수 확인(중복 제거 이전의 sentence length)\n",
        "sentence_lengths = []\n",
        "for sentence in X_kiwi_combined:\n",
        "    words = sentence.split()\n",
        "    sentence_length = len(words)\n",
        "    sentence_lengths.append(sentence_length)\n",
        "\n",
        "print(\"Total sentences:\", len(X_kiwi_combined))\n",
        "print(\"Sentence lengths:\", sentence_lengths)\n",
        "\n",
        "# X_kiwi_combined의 길이만큼 반복: 모든 단어의 중요도 출력\n",
        "for i in range(len(X_kiwi_combined)):\n",
        "    # 키워드 추출\n",
        "    keywords = kw_model.extract_keywords(X_kiwi_combined[i], keyphrase_ngram_range=(1, 1), stop_words=None, top_n=sentence_lengths[i])\n",
        "\n",
        "    # weight_set에 키워드 추가\n",
        "    weight_set.append(keywords)\n",
        "\n",
        "for i, value in enumerate(weight_set):\n",
        "    print(f\"value at index {i}: {value}\")\n",
        "\n",
        "#변수 초기화\n",
        "excel_data=[]\n",
        "\n",
        "# 결과 출력 및 데이터 저장\n",
        "for i in range(len(X_kiwi_combined)):\n",
        "    for j in range(len(weight_set[i])):\n",
        "        word = weight_set[i][j][0]\n",
        "        score = weight_set[i][j][1]\n",
        "        print(f\"인덱스 {i}, 순서 {j}: {word} - {score}\")\n",
        "\n",
        "        # 데이터 리스트에 추가\n",
        "        excel_data.append([word, score])\n",
        "\n",
        "# 엑셀 파일로 저장\n",
        "df_weight_set = pd.DataFrame(excel_data, columns=['단어', '중요도'])\n",
        "file_path = '/content/drive/MyDrive/4U/db/bert_auto.xlsx'\n",
        "with pd.ExcelWriter(file_path, engine='xlsxwriter') as writer:\n",
        "    df_kiwi.to_excel(writer, sheet_name='bert_auto', index=False)\n",
        "\n",
        "#weight_set에서 각 문자열을 가져와서 단어의 개수 확인(중복 제거)\n",
        "sentence_lengths = []\n",
        "\n",
        "#반복문 실행\n",
        "for i in range(len(X_kiwi_combined)):\n",
        "  sentence_length=len(weight_set[i])\n",
        "  sentence_lengths.append(sentence_length)\n",
        "\n",
        "print(sentence_lengths)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# 결과 저장을 위한 리스트 초기화\n",
        "y_value = []\n",
        "\n",
        "# 모든 리스트의 길이 중 최소값을 구함\n",
        "min_len = min(len(y1), len(y2), len(y3))\n",
        "\n",
        "# i 반복문\n",
        "for i in range(min_len):\n",
        "    # i번째 단어가 weight_set의 어떤 j에 포함되는지 찾음\n",
        "    current_y = [str(y1[i]), str(y2[i]), str(y3[i])]\n",
        "\n",
        "    # 현재 단어의 weight 값을 저장할 리스트 초기화\n",
        "    current_weights = []\n",
        "\n",
        "    # j 반복문: current_y에 포함된 단어가 있는 경우에 해당하는 weight 값을 current_weights에 추가\n",
        "    for j in range(sentence_lengths[i]):\n",
        "\n",
        "        if any(word in weight_set[i][j][0] for word in current_y):  #string 찾는과정\n",
        "            current_weights.append(weight_set[i][j][1])  #float받기\n",
        "\n",
        "    # current_weights의 길이가 3이 되도록 부족한 부분은 0으로 채움\n",
        "    current_weights += [0.0] * (3 - len(current_weights))\n",
        "\n",
        "    # current_weights를 y_value에 추가\n",
        "    y_value.append(current_weights)\n",
        "\n",
        "# y_value를 NumPy 배열로 변환\n",
        "y_value_array = np.array(y_value)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"y_value_array:\")\n",
        "for i in range(len(y_value_array)):\n",
        "  print(y_value_array[i])\n",
        "print(\"y_value_array shape:\", y_value_array.shape)\n",
        "\n",
        "#기존에 입력한 키워드 정답(y1,y2,y3)과 키위가 분해한 것이 조금 달라 결측치가 많음(향후 개선 필요)\n",
        "\n",
        "\n",
        "#4.fasttext로 단어간 유사도 측정\n",
        "\n",
        "!pip install fasttext\n",
        "!chmod +x your_script.sh\n",
        "!wget https://github.com/facebookresearch/fastText/archive/master.zip\n",
        "!unzip master.zip\n",
        "!mv fastText-master fastText\n",
        "!rm master.zip\n",
        "!cd fastText && make\n",
        "!mv fastText/fasttext ./fasttext\n",
        "!./fasttext skipgram -input data/bn.txt -output data/fasttext/bn -dim 300 -minCount 79\n",
        "!./fasttext skipgram -input data/your_text_file.txt -output data/fasttext/your_model_name -dim 300 -minCount 79\n",
        "\n",
        "!pip install gensim\n",
        "\n",
        "!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1V4rTx4yaAg0x1NY1MpNRY2Dp1nKeyOQ7\" > /dev/null\n",
        "!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1V4rTx4yaAg0x1NY1MpNRY2Dp1nKeyOQ7\" -o wiki_20190620_small.txt\n",
        "\n",
        "from gensim.models.fasttext import FastText\n",
        "import gensim.models.word2vec\n",
        "\n",
        "from gensim.models import FastText\n",
        "\n",
        "# X_kiwi가 리스트 형태로 이미 존재한다고 가정\n",
        "# 각 단어가 문자열로 저장되어 있다고 가정\n",
        "\n",
        "# FastText 모델 학습\n",
        "model = FastText(X_kiwi, min_count=10, vector_size=500, window=5)\n",
        "\n",
        "\n"
      ]
    }
  ]
}