{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2H7zRVckBZ9k"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('../')\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "from db.db_conn import *\n",
        "\n",
        "# DB\n",
        "config_file_path = './db/config.ini'\n",
        "pc_option = 'db_name'\n",
        "select_sql_query = 'select * from db_name.wikidata'\n",
        "\n",
        "# output\n",
        "save_path = './myprj/our_data' + '/'\n",
        "save_fname = 'entity_embedding_ours.vec'\n",
        "\n",
        "# connect to DB\n",
        "host_ip, user_id, user_pw, db_name = load_db_info_from_config(config_file_path, pc_option)  # get db information\n",
        "cursor, mydb = connect_to_DB(host_ip, user_id, user_pw, db_name)\n",
        "\n",
        "table_df_tmp = get_relation_df_w_columns(cursor, select_sql_query)\n",
        "table_df = table_df_tmp.iloc[:, :]\n",
        "table_df.sort_values(by=['id'], axis=0)\n",
        "\n",
        "print(\"DB Load\")\n",
        "\n",
        "wikiword_list = list(np.array(table_df['word'].tolist()))\n",
        "\n",
        "X_train = []\n",
        "for idx, row in table_df.iterrows():\n",
        "    exp_khaiii = row['exp_khaiii']\n",
        "\n",
        "    exp_khaiii_rm = re.sub('[^A-Za-z0-9가-힣+]', ' ', str(exp_khaiii))\n",
        "    exp_khaiii_list = exp_khaiii_rm.split()\n",
        "    X_train.append(exp_khaiii_list)\n",
        "print(\"X_train 길이 : \", len(X_train))\n",
        "\n",
        "model = Word2Vec(sentences = X_train, vector_size = 100, window = 3, min_count=3, workers = 3, sg = 1) # min_count=3\n",
        "model.save(save_path + 'word2vec_80000.model')\n",
        "print(\"min_count 3일 때 '코로나'와 관련있는 단어 : \", model.wv.most_similar('코로나'))\n",
        "\n",
        "f = open(save_path + save_fname, 'w')\n",
        "error_word_list = []\n",
        "for idx, row in table_df.iterrows():\n",
        "    try:\n",
        "        wikiid = row['id']\n",
        "        wikiword = row['word']\n",
        "\n",
        "        # print(wikiword)\n",
        "        model.wv[wikiword]\n",
        "\n",
        "        write_vec = wikiid\n",
        "        for num in model.wv[wikiword]:\n",
        "            write_vec += '\\t' + str(num)\n",
        "        write_vec += '\\n'\n",
        "        f.write(write_vec)\n",
        "\n",
        "    except KeyError as e:\n",
        "        error_word_list.append(wikiword)\n",
        "        continue\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        # pass\n",
        "f.close()\n",
        "ef = open(save_path + 'except_error_word_list', 'w')\n",
        "ef.write(str(error_word_list))\n",
        "ef.close()\n",
        "\n",
        "\n",
        "#p3_make_input/news_word_entity.py\n",
        "\n",
        "import sys\n",
        "sys.path.append('../')\n",
        "\n",
        "import re, pickle\n",
        "\n",
        "from db.db_conn import *\n",
        "\n",
        "# DB\n",
        "config_file_path = './db/config.ini'\n",
        "pc_option = 'db_name'\n",
        "select_sql_query = 'select * from db_name.wikidata'\n",
        "past_sql_query = 'select * from db_name.news_model_past'\n",
        "\n",
        "# output\n",
        "save_path = './myprj/our_data' + '/'\n",
        "save_fname = 'news_words_dict.pkl'\n",
        "save_h_fname = 'news_entities_dict.pkl'\n",
        "\n",
        "# connect to DB\n",
        "host_ip, user_id, user_pw, db_name = load_db_info_from_config(config_file_path, pc_option)  # get db information\n",
        "cursor, mydb = connect_to_DB(host_ip, user_id, user_pw, db_name)\n",
        "\n",
        "table_df_tmp = get_relation_df_w_columns(cursor, select_sql_query)\n",
        "table_df = table_df_tmp.iloc[:, :] # 여기 뉴스개수 14438\n",
        "\n",
        "entity_dict = table_df[['id', 'word']].set_index('word').T.to_dict() # id 가 value, word가 key\n",
        "\n",
        "news_model_past_df_tmp = get_relation_df_w_columns(cursor, past_sql_query)\n",
        "news_model_past_df = news_model_past_df_tmp.iloc[:500000, :]\n",
        "\n",
        "print(\"DB Load\")\n",
        "\n",
        "all_news_words = {}\n",
        "all_news_entities = {}\n",
        "no_in_wikipedia = []\n",
        "for idx, row in news_model_past_df.iterrows():\n",
        "    news_index = row['news_index']\n",
        "    word = row['word']\n",
        "    entity = row['entity']\n",
        "\n",
        "    word_rm = re.sub('[^A-Za-z0-9가-힣+]', ' ', str(word))\n",
        "    word_list = word_rm.split()\n",
        "\n",
        "    entity_rm = re.sub('[^A-Za-z0-9가-힣+]', ' ', str(entity))\n",
        "    entity_list = entity_rm.split()\n",
        "\n",
        "    ## news_words\n",
        "    all_news_words[news_index] = word_list\n",
        "\n",
        "    ## news_entities\n",
        "    entities_list = []\n",
        "    for e in entity_list:\n",
        "        one_entity = []\n",
        "        one_entity.append(e)\n",
        "\n",
        "        if e not in entity_dict:\n",
        "            continue\n",
        "        entity_id = entity_dict[e]['id']\n",
        "        tup = (one_entity, entity_id)\n",
        "\n",
        "        entities_list.append(tup)\n",
        "\n",
        "    all_news_entities[news_index] = entities_list\n",
        "\n",
        "\n",
        "with open(save_path + save_fname,'wb') as f: # 쓸 때 wb, 가져올 때 rb\n",
        "    pickle.dump(all_news_words,f)\n",
        "\n",
        "f.close()\n",
        "\n",
        "with open(save_path + save_h_fname,'wb') as ff:\n",
        "    pickle.dump(all_news_entities, ff)\n",
        "\n",
        "ff.close()\n",
        "\n",
        "no_in_wikipedia = list(set(no_in_wikipedia))\n",
        "with open(save_path + 'no_in_wikipedia','wb') as f:\n",
        "    pickle.dump(all_news_entities, f)\n",
        "\n",
        "#p3_make_input/session_history.py\n",
        "\n",
        "import sys\n",
        "sys.path.append('../')\n",
        "\n",
        "import random\n",
        "import re, pickle\n",
        "import pandas as pd\n",
        "\n",
        "from db.db_conn import *\n",
        "from module.entity_embedding_module import *\n",
        "\n",
        "# DB\n",
        "config_file_path = './db/config.ini'\n",
        "pc_option = 'db_name'\n",
        "select_sql_query = 'select * from db_name.user_history_dup'\n",
        "past_sql_query = 'select * from db_name.news_model_past'\n",
        "\n",
        "# output\n",
        "save_path = './myprj/our_data' + '/'\n",
        "save_fname = 'session_list.pkl'\n",
        "save_h_fname = 'history_dict.pkl'\n",
        "\n",
        "# connect to DB\n",
        "host_ip, user_id, user_pw, db_name = load_db_info_from_config(config_file_path, pc_option)  # get db information\n",
        "cursor, mydb = connect_to_DB(host_ip, user_id, user_pw, db_name)\n",
        "\n",
        "table_df_tmp = get_relation_df_w_columns(cursor, select_sql_query)\n",
        "table_df = table_df_tmp.iloc[:, :] # 얘네 뉴스개수 14438\n",
        "\n",
        "news_model_past_df_tmp = get_relation_df_w_columns(cursor, past_sql_query)\n",
        "news_model_past_df = news_model_past_df_tmp.iloc[:500000, :]\n",
        "\n",
        "print(\"DB Load\")\n",
        "\n",
        "table_groupby = table_df.groupby('user_id')['news_index'].apply(lambda x: \"[%s]\" % ', '.join(x))\n",
        "table_groupby_df = table_groupby.reset_index(drop=False)\n",
        "\n",
        "all_news_index = list(set(list(news_model_past_df['news_index'])))\n",
        "\n",
        "rdlist_3 = [1, 2, 3]\n",
        "rdlist_1 = [0, 1]\n",
        "rdlist_7 = [7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
        "all_session_list = []\n",
        "all_history_dict = {}\n",
        "for idx, row in table_groupby_df.iterrows():\n",
        "    user_session_list = []\n",
        "    user_id = row['user_id']\n",
        "    news_index = row['news_index']\n",
        "\n",
        "    news_index_rm = re.sub('[^A-Za-z0-9가-힣+]', ' ', str(news_index))\n",
        "    news_index_list = news_index_rm.split()\n",
        "\n",
        "    ## history\n",
        "    all_history_dict[user_id] = news_index_list\n",
        "\n",
        "    ## session\n",
        "    # 1. user\n",
        "    user_session_list.append(user_id)\n",
        "\n",
        "    # 2. user_history(uh)\n",
        "    # 3. user에게 노출된 뉴스 중 1번 이상 클릭한 기사들(oneclick_list)\n",
        "    if len(news_index_list) > 25:\n",
        "        oneclick_list = random.sample(news_index_list, 6)\n",
        "    elif len(news_index_list) > 10:\n",
        "        rdnum = int(random.sample(rdlist_3, 1)[0])\n",
        "        oneclick_list = random.sample(news_index_list, rdnum)\n",
        "    elif len(news_index_list) > 3:\n",
        "        rdnum = int(random.sample(rdlist_1, 1)[0])\n",
        "        oneclick_list = random.sample(news_index_list, rdnum)\n",
        "    else:\n",
        "        oneclick_list = []\n",
        "\n",
        "    uh = [x for x in news_index_list if x not in oneclick_list]\n",
        "\n",
        "    user_session_list.append(uh)\n",
        "    user_session_list.append(oneclick_list)\n",
        "\n",
        "    rm = []\n",
        "    # 4. user에게 노출되었으나, 클릭하지 않는 기사들(noclick_list)\n",
        "    rdnum = int(random.sample(rdlist_7, 1)[0])\n",
        "    noclick_list = random.sample(all_news_index, rdnum)\n",
        "    for noclick_elt in noclick_list:\n",
        "        if noclick_elt in news_index_list:\n",
        "            rm.append(noclick_elt)\n",
        "    noclick_list = [x for x in noclick_list if x not in rm]\n",
        "    user_session_list.append(noclick_list)\n",
        "\n",
        "    # 전체에 append\n",
        "    all_session_list.append(user_session_list)\n",
        "\n",
        "\n",
        "with open(save_path + save_fname,'wb') as f: # 쓸 때 wb, 가져올 때 rb\n",
        "    pickle.dump(all_session_list,f)\n",
        "\n",
        "f.close()\n",
        "\n",
        "with open(save_path + save_h_fname,'wb') as ff:\n",
        "    pickle.dump(all_history_dict, ff)\n",
        "\n",
        "ff.close()"
      ]
    }
  ]
}